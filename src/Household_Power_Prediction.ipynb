{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ireUoEJVkZCf"
   },
   "source": [
    "# <center>Household Power Consumption Prediction<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4pv3pNZkZCh"
   },
   "source": [
    "### Description \n",
    "This notebook implements a an sequential model from Household Power Consumption Prediction. Model input are measurements of electric power consumption. Data was gathered from one household with a one-minute sampling rate over a period of almost 4 years.\n",
    "\n",
    "#### Data Set Information:\n",
    "\n",
    "The dataset contains 2075259 samples/measurements gathered between December 2006 and November 2010 (47 months).<br>\n",
    "**NOTE**: \n",
    "> (global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\n",
    "\n",
    "> The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\n",
    "\n",
    "**Data Attributes Information:** \n",
    "\n",
    "1. date: Date in format dd/mm/yyyy\n",
    "\n",
    "2. time: time in format hh:mm:ss\n",
    "\n",
    "3. global_active_power: household global minute-averaged active power (in kilowatt)\n",
    "\n",
    "4. global_reactive_power: household global minute-averaged reactive power (in kilowatt)\n",
    "\n",
    "5. voltage: minute-averaged voltage (in volt)\n",
    "\n",
    "6. global_intensity: household global minute-averaged current intensity (in ampere)\n",
    "\n",
    "7. sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n",
    "\n",
    "8. sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n",
    "\n",
    "9. sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner. <br>\n",
    "\n",
    "#### Task : \n",
    "> Predict the power consumption together with upper and lower interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Zvape09kZCi"
   },
   "source": [
    "### Import necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPwyHooekZCl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0c9np2jWkZCs"
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5LHzd4MEkZCu"
   },
   "outputs": [],
   "source": [
    "data_path = \"../Dataset/household_power_consumption_data.zip\"\n",
    "df = pd.read_csv(data_path, sep=';',parse_dates={'dt' : ['Date', 'Time']}, infer_datetime_format=True,\n",
    "                 low_memory=False, na_values=['nan','?'],index_col='dt')\n",
    "df.drop([\"Global_active_power\",\"Global_reactive_power\",\"Voltage\",\"Global_intensity\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "brZk7BhIkZDC"
   },
   "source": [
    "### Preprocess Data\n",
    "Fill NaN values by average, extract features from timestamps & group the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill nan values with column average\n",
    "for j in range(0,3):\n",
    "    df.iloc[:,j]=df.iloc[:,j].fillna(df.iloc[:,j].mean())\n",
    "\n",
    "df[\"consumption\"] = df.iloc[:,:].sum(axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features from timestamp \n",
    "def features_from_timestamp(t):\n",
    "    h = t.hour\n",
    "    year = t.year\n",
    "    idx = np.searchsorted(list(range(3,25,3)),h,side='right')\n",
    "    interval = np.arange(3,25,3)[idx]\n",
    "    if  interval == 24 : interval = 0\n",
    "    month = t.month\n",
    "    season = (month in [11,12,1,2,3]) * 1 # 0: summer, 1:  winter\n",
    "    return [t.day, t.dayofweek,h,interval,month,season]\n",
    "\n",
    "\n",
    "col = ['day', 'dayofweek', 'hour', 'interval', 'month', 'season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df[\"consumption\"].groupby(pd.Grouper(freq='1h', base=0, label='right')).sum()\n",
    "data = pd.DataFrame(grouped/60)\n",
    "\n",
    "#Get intervals\n",
    "\n",
    "# Generate aditional features from timestamp then append to exixting data\n",
    "additional_featues = pd.DataFrame(data = [features_from_timestamp(i) for i in tqdm(data.index) ],columns=col).set_index(data.index)\n",
    "data = data.merge(additional_featues,on=\"dt\")\n",
    "data.sort_index(inplace=True) #make sure data is sorted by date\n",
    "\n",
    "for col in list(data.columns)[1:]:\n",
    "    data[col] = data[col].astype(\"category\")\n",
    "cate_features_name = ['day', 'dayofweek', 'hour', 'interval', 'month', 'season']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset with upper bound and lower bound\n",
    "For each record look back one month and calculate the standard deviation, quantile (5% and 95%) and the mean energy consumption. Check if each record is between the upper bound and lower bound (5 and 95 percent quantile in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame(grouped/60)\n",
    "def get_intervals(t):\n",
    "    global data2\n",
    "    lookback = t - pd.Timedelta('730hr')\n",
    "    day_name = t.day_name()\n",
    "    mask = ((data2.index >= lookback) & (data2.index <= t))\n",
    "\n",
    "    if np.sum(mask) < 144:\n",
    "        return [np.nan, np.nan,np.nan,np.nan]\n",
    "\n",
    "    d = data[mask].groupby(data[mask].index.weekday_name)\n",
    "#     temp = d.get_group(day_name).values\n",
    "    temp = d.get_group(day_name)\n",
    "    temp = temp.groupby(temp.index.hour)\n",
    "    k = t.hour if len(temp.groups.keys()) == 24 else t.hour - 1\n",
    "    if k == -1 :\n",
    "        k = 23\n",
    "    \n",
    "    temp = temp.get_group(k).values\n",
    "\n",
    "    p1 = np.quantile(temp,0.05)\n",
    "    p2 = np.quantile(temp,0.95)\n",
    "    \n",
    "    return [temp.std() ,temp.mean(), p1, p2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_col = [\"std\",\"mean\",\"p1\",\"p2\"]\n",
    "addi = pd.DataFrame(data = [get_intervals(i) for i in tqdm(data2.index)],columns=add_col).set_index(data2.index)\n",
    "data2 = data2.merge(addi,on=\"dt\")\n",
    "data2.dropna(inplace=True)\n",
    "data2['in_bounds'] = data2[\"consumption\"].between(left=data2['p1'], right=data2['p2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.in_bounds.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data to train and test\n",
    "Take one year as test sample (2010) and 3 years as trainning set (2006 - 2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data to train and test\n",
    "xtrain = data.loc[\"2006\":\"2010\"]\n",
    "ytrain = xtrain.pop(\"consumption\")\n",
    "\n",
    "xtest = data.loc[\"2010\":]\n",
    "ytest = xtest.pop(\"consumption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models & Train models\n",
    "Define and set modes training paramaters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use sklearn Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_ALPHA = 0.10\n",
    "UPPER_ALPHA = 0.90\n",
    "\n",
    "N_ESTIMATORS = 700\n",
    "MAX_DEPTH = 5\n",
    "\n",
    "# Each model has to be separate\n",
    "lower_model = GradientBoostingRegressor(loss=\"quantile\", alpha=LOWER_ALPHA, n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH)\n",
    "# The mid model will use the default loss\n",
    "mid_model = GradientBoostingRegressor(loss=\"ls\", n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH)\n",
    "upper_model = GradientBoostingRegressor(loss=\"quantile\", alpha=UPPER_ALPHA, n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH)\n",
    "\n",
    "\n",
    "#Train Models\n",
    "_ = lower_model.fit(xtrain, ytrain)\n",
    "_ = mid_model.fit(xtrain, ytrain)\n",
    "_ = upper_model.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(mid_model.predict(xtest),ytest)\n",
    "print(\"Gradient Boosting Regressor MSE : {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\"task\":\"train\",\"max_depth\": -1, \"objective\": \"quantile\",\"learning_rate\" : 0.1,\n",
    "#           \"alpha\":0.5 ,\"num_leaves\": 900,'metric': {'l2', 'auc'},  \"n_estimators\": 700,\n",
    "#          \"n_jobs\" : -1}\n",
    "\n",
    "# d_train = lgb.Dataset(xtrain, label=ytrain, categorical_feature = cate_features_name)\n",
    "# model2 = lgb.train(params, d_train)\n",
    "\n",
    "lower_model_lgbm = lgb.LGBMRegressor(objective=\"quantile\",alpha=0.05,n_estimators=700,max_depth=15,n_jobs=-1, learning_rate= 0.1,\n",
    "                                   num_leaves= 900)\n",
    "mid_model_lgbm = lgb.LGBMRegressor(objective=\"quantile\",alpha=0.5,n_estimators=700,max_depth=15,n_jobs=-1, learning_rate= 0.1,\n",
    "                                   num_leaves= 900)\n",
    "upper_model_lgbm = lgb.LGBMRegressor(objective=\"quantile\",alpha=0.95,n_estimators=700,max_depth=15,n_jobs=-1, learning_rate= 0.1,\n",
    "                                     num_leaves= 900)\n",
    "#Train Models\n",
    "lower_model_lgbm.fit(xtrain, ytrain,verbose = 0,categorical_feature=cate_features_name,eval_metric = ['l2', 'auc'])\n",
    "mid_model_lgbm.fit(xtrain, ytrain,verbose = 0,categorical_feature=cate_features_name,eval_metric = ['l2', 'auc'])\n",
    "upper_model_lgbm.fit(xtrain, ytrain,verbose = 0,categorical_feature=cate_features_name,eval_metric = ['l2', 'auc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(mid_model_lgbm.predict(xtest),ytest)\n",
    "print(\"LightGBM MSE : {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(ytest)\n",
    "predictions['lower'] = lower_model_lgbm.predict(xtest)\n",
    "predictions['mid'] = mid_model_lgbm.predict(xtest)\n",
    "predictions['upper'] = upper_model_lgbm.predict(xtest)\n",
    "\n",
    "assert (predictions['upper'] > predictions['lower']).all()\n",
    "\n",
    "predictions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clip negative values to zero\n",
    "predictions.loc[predictions['lower'] < 0.0, \"lower\"] = 0.0\n",
    "predictions.loc[predictions['mid'] < 0.0, \"mid\"] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate model Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(predictions):\n",
    "    predictions['absolute_error_lower'] = (predictions['lower'] - predictions[\"consumption\"]).abs()\n",
    "    predictions['absolute_error_upper'] = (predictions['upper'] - predictions[\"consumption\"]).abs()\n",
    "    \n",
    "    predictions['absolute_error_interval'] = (predictions['absolute_error_lower'] + predictions['absolute_error_upper']) / 2\n",
    "    predictions['absolute_error_mid'] = (predictions['mid'] - predictions[\"consumption\"]).abs()\n",
    "    \n",
    "    predictions['in_bounds'] = predictions[\"consumption\"].between(left=predictions['lower'], right=predictions['upper'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_error(predictions)\n",
    "metrics = predictions[['absolute_error_lower', 'absolute_error_upper', 'absolute_error_interval', 'absolute_error_mid', 'in_bounds']].copy()\n",
    "metrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.in_bounds.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "n = 100\n",
    "fig = plt.figure(figsize=(19.20,10.80))\n",
    "# plt.plot(predictions.index[100:150], predictions.lower[100:150],label=\"Lower\",linewidth=3)\n",
    "plt.plot(predictions.index[:n], predictions.mid[:n],label=\"Prediction\",linewidth=3)\n",
    "# plt.plot(predictions.index[100:150], predictions.upper[100:150], label=\"Upper\",linewidth=3)\n",
    "plt.fill_between(predictions.index[:n], predictions.lower[:n], predictions.upper[:n],\n",
    "                 alpha=0.3, facecolor=colors[8],label='interval')\n",
    "plt.plot(predictions.index[:n], predictions.consumption[:n],label=\"actual\",linewidth=3)\n",
    "plt.legend(loc=0,prop={'size': 20})\n",
    "plt.ylabel(\"Energy Consumption (Wh)\",fontsize=20.0,fontweight=\"bold\")\n",
    "plt.xlabel(\"Time\",fontsize=20.0,fontweight=\"bold\")\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jiqyu_iMkZC0"
   },
   "source": [
    "### Exploratory Data Analysis (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "joBfJXorkZC2"
   },
   "outputs": [],
   "source": [
    "# import pandas_profiling\n",
    "# profile = pandas_profiling.ProfileReport(df)\n",
    "# profile.to_file(\"report.html\")\n",
    "# profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Predictions of intervals using Deep Quantile Regression (Deep Neural network) <center/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data\n",
    "scaler = StandardScaler()\n",
    "std_xtrain = scaler.fit_transform(xtrain.values)\n",
    "std_xtest = scaler.transform(xtest.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(q,y,f):\n",
    "    # q: Quantile to be evaluated, e.g., 0.5 for median.\n",
    "    # y: True value.\n",
    "    # f: Fitted (predicted) value.\n",
    "    e = (y-f)\n",
    "    return K.mean(K.maximum(q*e, (q-1)*e), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BaseModel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=6,activation='relu'))\n",
    "    model.add(Dense(units=10, input_dim=1,activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "model_lower = BaseModel()\n",
    "model_lower.compile(loss=lambda y,f: quantile_loss(0.1,y,f), optimizer='adadelta')\n",
    "model_lower.fit(std_xtrain, ytrain, epochs=200, batch_size=32, verbose=0,validation_split=0.2, callbacks=[early_stop])\n",
    "\n",
    "model_middle = BaseModel()\n",
    "model_middle.compile(loss=lambda y,f: quantile_loss(0.5,y,f), optimizer='adadelta')\n",
    "model_middle.fit(std_xtrain, ytrain, epochs=200, batch_size=32, verbose=0,validation_split=0.2, callbacks=[early_stop])\n",
    "\n",
    "model_upper = BaseModel()\n",
    "model_upper.compile(loss=lambda y,f: quantile_loss(0.9,y,f), optimizer='adadelta')\n",
    "model_upper.fit(std_xtrain, ytrain, epochs=200, batch_size=32, verbose=0,validation_split=0.2, callbacks=[early_stop])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Household_Power_Prediction.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
